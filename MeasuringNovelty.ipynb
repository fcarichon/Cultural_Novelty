{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12595836-c97f-41b7-8caa-a3fe1f2c242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import json\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from utils import data_analysis, pmi, pmi_to_dict, docs_distribution, new_distribution\n",
    "from Scoring import compute_scores\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceaf360-3b9f-40fb-a50d-2648823241b1",
   "metadata": {},
   "source": [
    "### Fichier qui créé mes inputs (Proba dist for all, for each doc, and for new ones, and PMI calculations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1edc696-612b-4a8b-90fa-0fd3f3d0bd4d",
   "metadata": {},
   "source": [
    "Si j'ai des textes, ça doit me ressortir des vecteurs de distributions pour mes documents KB, EB, et pour mes nouveaux et les combinaisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e4f6e-d12a-4d68-86c0-d5e1a201ddab",
   "metadata": {},
   "source": [
    "## Pour chaque recette on compute la KB et la EB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664b8e0f-ca7a-47e7-ae94-a9abe8477a19",
   "metadata": {},
   "source": [
    "thresholds: </br>\n",
    "\n",
    "Novelty divergence threshold to know if a term significantly contribute (avg = 0.000923, std = 0.00161) -- set to : 0.0041 </br>\n",
    "Novelty divergence threshold to know if a term significantly contribute in probability of appearing (avg = 14.640, std = 21.253) -- set to : 57.14 </br>\n",
    "\n",
    "Novelty newness div threshold (avg = 9.787e-05, std = 0.0006) -- set to : 0.0014 </br>\n",
    "Novelty newness prob threshold (avg = 0.000601, std = 0.00043) -- set to : 0.0014 </br>\n",
    "Novelty uniqness dist threshold (avg = 0.341, std = 0.093) -- set to : 0.527 </br>\n",
    "Novelty uniqness shift threshold (avg = 0.0755, std = 0.0270) -- set to : 0.1295 </br>\n",
    "Novelty difference local threshold (avg = 0.0104, std = 0.0730) -- set to : 0.1564 </br>\n",
    "Novelty difference global threshold (avg = 0.1099, std = 0.1539) -- set to : 0.4177 </br>\n",
    "Novelty surprise new threshold (avg = 0.0016, std = 0.0052) -- set to : 0.0104 </br>\n",
    "Novelty surprise dist threshold (avg = 0.00048, std = 0.00104) -- set to : 0.00256 </br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813be516-80a4-4659-a0cc-10f73b39857a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "my_path = './Recipe_dataset/RecipeFullDataset/'\n",
    "my_new_path = './Recipe_dataset/Recipe_with_scores/'\n",
    "save_path = './Recipe_dataset/Recipe_with_scores/'\n",
    "filenames1 = set(next(walk(my_path), (None, None, []))[2])\n",
    "filenames2 = set(next(walk(my_new_path), (None, None, []))[2])\n",
    "\n",
    "filenames = list(filenames1 - filenames2)\n",
    "print(len(filenames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81f61635-6fc0-40a7-9964-1c5804504b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Marinade_recipe_2.json',\n",
       " 'Tartar_recipe_4.json',\n",
       " 'Sesame Noodle_recipe_1.json',\n",
       " 'Honey Cake_recipe_0.json',\n",
       " 'Chicken Satay_recipe_3.json',\n",
       " 'Chile Relleno_recipe_236.json']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dee02c93-befc-4f3d-beb6-5a1a903bc231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base size :  34\n",
      "train variations for recipe Marinade_recipe_2.json done  | train variation size :  211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 211/211 [02:19<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid variations for recipe Marinade_recipe_2.json done  | valid variation size :  34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 34/34 [00:21<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test variations for recipe Marinade_recipe_2.json done  | test variation size :  42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 42/42 [00:27<00:00,  1.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 0 saved :  Marinade_recipe_2.json\n",
      "Knowledge base size :  4\n",
      "train variations for recipe Tartar_recipe_4.json done  | train variation size :  14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 14/14 [00:01<00:00,  7.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid variations for recipe Tartar_recipe_4.json done  | valid variation size :  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  7.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test variations for recipe Tartar_recipe_4.json done  | test variation size :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  6.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 1 saved :  Tartar_recipe_4.json\n",
      "Knowledge base size :  14\n",
      "train variations for recipe Sesame Noodle_recipe_1.json done  | train variation size :  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  3.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid variations for recipe Sesame Noodle_recipe_1.json done  | valid variation size :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test variations for recipe Sesame Noodle_recipe_1.json done  | test variation size :  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:01<00:00,  3.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 2 saved :  Sesame Noodle_recipe_1.json\n",
      "Knowledge base size :  8\n",
      "train variations for recipe Honey Cake_recipe_0.json done  | train variation size :  29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 29/29 [00:07<00:00,  4.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid variations for recipe Honey Cake_recipe_0.json done  | valid variation size :  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  4.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test variations for recipe Honey Cake_recipe_0.json done  | test variation size :  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:01<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 3 saved :  Honey Cake_recipe_0.json\n",
      "Knowledge base size :  29\n",
      "train variations for recipe Chicken Satay_recipe_3.json done  | train variation size :  12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 12/12 [00:11<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid variations for recipe Chicken Satay_recipe_3.json done  | valid variation size :  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test variations for recipe Chicken Satay_recipe_3.json done  | test variation size :  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:07<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 4 saved :  Chicken Satay_recipe_3.json\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting property name enclosed in double quotes: line 29 column 9 (char 2407)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m file_path \u001b[38;5;241m=\u001b[39m my_path \u001b[38;5;241m+\u001b[39m recette\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path) \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m----> 6\u001b[0m     recipe_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_file)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#### COLLECTING ALL NECESSARY INFO\u001b[39;00m\n\u001b[0;32m      9\u001b[0m KB_recettes, _ \u001b[38;5;241m=\u001b[39m data_analysis(recipe_dict)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:293\u001b[0m, in \u001b[0;36mload\u001b[1;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(fp, \u001b[38;5;241m*\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    275\u001b[0m         parse_int\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, parse_constant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, object_pairs_hook\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    276\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Deserialize ``fp`` (a ``.read()``-supporting file-like object containing\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;124;03m    a JSON document) to a Python object.\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m    kwarg; otherwise ``JSONDecoder`` is used.\u001b[39;00m\n\u001b[0;32m    292\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loads(fp\u001b[38;5;241m.\u001b[39mread(),\n\u001b[0;32m    294\u001b[0m         \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcls\u001b[39m, object_hook\u001b[38;5;241m=\u001b[39mobject_hook,\n\u001b[0;32m    295\u001b[0m         parse_float\u001b[38;5;241m=\u001b[39mparse_float, parse_int\u001b[38;5;241m=\u001b[39mparse_int,\n\u001b[0;32m    296\u001b[0m         parse_constant\u001b[38;5;241m=\u001b[39mparse_constant, object_pairs_hook\u001b[38;5;241m=\u001b[39mobject_pairs_hook, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[1;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_decoder\u001b[38;5;241m.\u001b[39mdecode(s)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[1;34m(self, s, _w)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[0;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[0;32m    335\u001b[0m \n\u001b[0;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_decode(s, idx\u001b[38;5;241m=\u001b[39m_w(s, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mend())\n\u001b[0;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\json\\decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[1;34m(self, s, idx)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscan_once(s, idx)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mJSONDecodeError\u001b[0m: Expecting property name enclosed in double quotes: line 29 column 9 (char 2407)"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "for k in range(len(filenames)):\n",
    "    recette = filenames[k]\n",
    "    file_path = my_path + recette\n",
    "    with open(file_path) as json_file:\n",
    "        recipe_dict = json.load(json_file)\n",
    "\n",
    "    #### COLLECTING ALL NECESSARY INFO\n",
    "    KB_recettes, _ = data_analysis(recipe_dict)\n",
    "    print('Knowledge base size : ', len(KB_recettes))\n",
    "    if len(KB_recettes) <= 0:\n",
    "        continue\n",
    "\n",
    "    ### Transforming KB into distribution\n",
    "    KB_texts = ' '.join(KB_recettes).split()\n",
    "    EB_PMI = pmi(KB_texts)\n",
    "    dict_know_pmi = pmi_to_dict(EB_PMI)\n",
    "    \n",
    "    train_recettes, train_indexes  = data_analysis(recipe_dict, ref=False, col_name='Train_Variations')\n",
    "    valid_recettes, valid_indexes  = data_analysis(recipe_dict, ref=False, col_name='Valid_Variations')\n",
    "    test_recettes, test_indexes  = data_analysis(recipe_dict, ref=False, col_name='Test_Variations')\n",
    "    \n",
    "    recette_variations = train_recettes + valid_recettes + test_recettes\n",
    "    KB_matrix,  KB_dist, Count_matrix = docs_distribution(KB_recettes, recette_variations)\n",
    "    KB_size = list(range(KB_matrix.shape[0]))\n",
    "    \n",
    "    ## We set to 0 the distance here for each recipe -- difference needs to estimate distance between all points. \n",
    "    #This serves as optim to not calculate for each varaitions but only once since it is the same distance for all KB\n",
    "    print('train variations for recipe {} done'.format(recette), ' | train variation size : ', len(train_recettes))\n",
    "    neighboroud_distance  = 0. \n",
    "    for i in tqdm(range(len(train_recettes))):\n",
    "        select_variation = KB_size + [len(KB_size)+i]\n",
    "        NewKB_dist, variation_dist = new_distribution(Count_matrix, select_variation)\n",
    "        \n",
    "        KB_updated = KB_recettes + [train_recettes[i]]\n",
    "        updated_text = ' '.join(KB_updated).split()\n",
    "        New_EB_PMI = pmi(updated_text)\n",
    "        \n",
    "        #return newness, novelty_new, uniqueness, novelty_uniq, diff_ratio, nolvety_diff, neighbor_dist, newratio_surprise_rate, newn_suprise, dist_surprise, uniq_surprise\n",
    "        newness, novelty_new, uniqueness, novelty_uniq, difference, \\\n",
    "        nolvety_diff, neighboroud_distance, new_surprise, novelty_nsuprise, \\\n",
    "        dist_surprise, novelty_dsurprise = compute_scores(KB_matrix, KB_dist, NewKB_dist, variation_dist, EB_PMI, dict_know_pmi, New_EB_PMI, neighbor_dist=neighboroud_distance)\n",
    "\n",
    "        current_index = train_indexes[i]\n",
    "        recipe_dict['Train_Variations'][current_index]['newness'] = newness\n",
    "        recipe_dict['Train_Variations'][current_index]['novelty_new'] = novelty_new\n",
    "        recipe_dict['Train_Variations'][current_index]['uniqueness'] = uniqueness\n",
    "        recipe_dict['Train_Variations'][current_index]['novelty_uniq'] = novelty_uniq\n",
    "        recipe_dict['Train_Variations'][current_index]['difference'] = difference\n",
    "        recipe_dict['Train_Variations'][current_index]['nolvety_diff'] = nolvety_diff\n",
    "        recipe_dict['Train_Variations'][current_index]['new_surprise'] = new_surprise\n",
    "        recipe_dict['Train_Variations'][current_index]['novelty_nsuprise'] = novelty_nsuprise\n",
    "        recipe_dict['Train_Variations'][current_index]['dist_surprise'] = dist_surprise\n",
    "        recipe_dict['Train_Variations'][current_index]['novelty_dsurprise'] = novelty_dsurprise\n",
    "    \n",
    "    print('valid variations for recipe {} done'.format(recette), ' | valid variation size : ', len(valid_recettes))\n",
    "    for i in tqdm(range(len(valid_recettes))):\n",
    "\n",
    "        select_variation = KB_size + [(len(KB_size)+len(train_recettes))+i]\n",
    "        NewKB_dist, variation_dist = new_distribution(Count_matrix, select_variation)\n",
    "\n",
    "        KB_updated = KB_recettes + [valid_recettes[i]]\n",
    "        updated_text = ' '.join(KB_updated).split()\n",
    "        New_EB_PMI = pmi(updated_text)\n",
    "        \n",
    "        newness, novelty_new, uniqueness, novelty_uniq, diff_ratio, \\\n",
    "        nolvety_diff, neighboroud_distance, newratio_surprise_rate, newn_suprise, \\\n",
    "        dist_surprise, uniq_surprise = compute_scores(KB_matrix, KB_dist, NewKB_dist, variation_dist, EB_PMI, dict_know_pmi, New_EB_PMI, neighbor_dist=neighboroud_distance)\n",
    "        current_index = valid_indexes[i]\n",
    "        recipe_dict['Valid_Variations'][current_index]['newness'] = newness\n",
    "        recipe_dict['Valid_Variations'][current_index]['novelty_new'] = novelty_new\n",
    "        recipe_dict['Valid_Variations'][current_index]['uniqueness'] = uniqueness\n",
    "        recipe_dict['Valid_Variations'][current_index]['novelty_uniq'] = novelty_uniq\n",
    "        recipe_dict['Valid_Variations'][current_index]['difference'] = difference\n",
    "        recipe_dict['Valid_Variations'][current_index]['nolvety_diff'] = nolvety_diff\n",
    "        recipe_dict['Valid_Variations'][current_index]['new_surprise'] = new_surprise\n",
    "        recipe_dict['Valid_Variations'][current_index]['novelty_nsuprise'] = novelty_nsuprise\n",
    "        recipe_dict['Valid_Variations'][current_index]['dist_surprise'] = dist_surprise\n",
    "        recipe_dict['Valid_Variations'][current_index]['novelty_dsurprise'] = novelty_dsurprise\n",
    "    \n",
    "    print('test variations for recipe {} done'.format(recette), ' | test variation size : ', len(test_recettes))\n",
    "    for i in tqdm(range(len(test_recettes))):\n",
    "\n",
    "        select_variation = KB_size + [(len(KB_size)+len(train_recettes)+len(valid_recettes))+i]\n",
    "        NewKB_dist, variation_dist = new_distribution(Count_matrix, select_variation)\n",
    "\n",
    "        KB_updated = KB_recettes + [test_recettes[i]]\n",
    "        updated_text = ' '.join(KB_updated).split()\n",
    "        New_EB_PMI = pmi(updated_text)\n",
    "        \n",
    "        newness, novelty_new, uniqueness, novelty_uniq, diff_ratio, \\\n",
    "        nolvety_diff, neighboroud_distance, newratio_surprise_rate, newn_suprise, \\\n",
    "        dist_surprise, uniq_surprise = compute_scores(KB_matrix, KB_dist, NewKB_dist, variation_dist, EB_PMI, dict_know_pmi, New_EB_PMI, neighbor_dist=neighboroud_distance)\n",
    "        current_index = test_indexes[i]\n",
    "        recipe_dict['Test_Variations'][current_index]['newness'] = newness\n",
    "        recipe_dict['Test_Variations'][current_index]['novelty_new'] = novelty_new\n",
    "        recipe_dict['Test_Variations'][current_index]['uniqueness'] = uniqueness\n",
    "        recipe_dict['Test_Variations'][current_index]['novelty_uniq'] = novelty_uniq\n",
    "        recipe_dict['Test_Variations'][current_index]['difference'] = difference\n",
    "        recipe_dict['Test_Variations'][current_index]['nolvety_diff'] = nolvety_diff\n",
    "        recipe_dict['Test_Variations'][current_index]['new_surprise'] = new_surprise\n",
    "        recipe_dict['Test_Variations'][current_index]['novelty_nsuprise'] = novelty_nsuprise\n",
    "        recipe_dict['Test_Variations'][current_index]['dist_surprise'] = dist_surprise\n",
    "        recipe_dict['Test_Variations'][current_index]['novelty_dsurprise'] = novelty_dsurprise\n",
    "    \n",
    "    file_name = save_path + recette\n",
    "    with open(file_name, \"w\") as outfile:\n",
    "        json.dump(recipe_dict, outfile)\n",
    "\n",
    "    print('file {} saved : '.format(k), recette)\n",
    "    # Reset the start time to measure time per iteration\n",
    "    start_time = time.time()\n",
    "    del recipe_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495dc6f5-8b95-457f-a37d-0cccc2f4ce11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
