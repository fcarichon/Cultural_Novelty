{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362d0c7-e344-4926-a9fc-5b17291cb163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "from collections import Counter\n",
    "import re\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict \n",
    "from collections import Counter\n",
    "from os import walk\n",
    "\n",
    "from scipy import stats\n",
    "import math\n",
    "import statistics\n",
    "\n",
    "from scipy import stats\n",
    "import math\n",
    "import statistics\n",
    "from scipy.spatial import distance\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from Stats_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4670b31-2e5c-494b-a014-573a9da792bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = '.'\n",
    "my_path = f'./datasets/GlobalFusion/Recipes_with_scores/'\n",
    "filenames = list(set(next(walk(my_path), (None,None,[]))[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cde92-4c19-4cae-a1a4-a16394bda799",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING CULTURAL DISTANCES DATASETS\n",
    "df_ingle = pd.read_excel(f'{ROOT_DIR}/datasets/Cultural_Datasets/CulturalDistanceEVSWVS.xlsx')\n",
    "df_ingle = df_ingle.dropna(subset=['Country'])\n",
    "dist_ingle = inglehart_dist(df_ingle)\n",
    "\n",
    "df_geo = pd.read_excel(f'{ROOT_DIR}/datasets/Cultural_Datasets/Country_latLong.xlsx')\n",
    "df_geo = df_geo.dropna(subset=['Country'])\n",
    "dist_geo = geograph_dist(df_geo)\n",
    "\n",
    "dist_ling = pd.read_excel(f'{ROOT_DIR}/datasets/Cultural_Datasets/Lang-Dist_all-years_n22350.xlsx')\n",
    "dist_rel = pd.read_csv(f'{ROOT_DIR}/datasets/Cultural_datasets/religious_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54a1c3e-3a5d-4484-b225-3daeeda02340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing all metrics we want to save for correlation and logistic regression analyses\n",
    "all_new_div, all_new_prob, all_new_extr, all_new_rank = [], [], [], []\n",
    "all_uniq_dist, all_uniq_proto, all_diff_glob, all_diff_loc = [], [], [], []\n",
    "all_nsurp, all_dsurp = [], []\n",
    "all_dist_ingle, all_dist_ling, all_dist_geo, all_dist_rel = [], [], [], []\n",
    "nb_ingr, nb_new_ingr, ratio_new_ingr, clean_text_length, clean_nb_uniq_tokens, lexical_div, ratio_length = [], [], [], [], [], [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0b09a-ac50-4dff-acc4-fbc1350fa8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR LINGUISTIC OR RELIGIOUS DISTANCE\n",
    "country_list_1 = list(set(df['country_i']))\n",
    "#FOR INGLEHART CULTURAl or GEOGRAPHICAL DISTANCE\n",
    "country_list_1 = list(df['Country'])\n",
    "\n",
    "for k in tqdm(range(len(filenames))):\n",
    "    filename = filenames[k]\n",
    "    file_path = my_path + filename\n",
    "    with open(file_path) as json_file:\n",
    "        recipe_dict1 = json.load(json_file)\n",
    "\n",
    "    KB_size = df.iloc[k]['Nb recipes in KB']\n",
    "    KB_avg_length = get_KB_size(recipe_dict1['Reference_Base'])\n",
    "    \n",
    "    country = recipe_dict1['Country']\n",
    "    ##Exceptions due to the Congo Bars recipes\n",
    "    if country == 'congo':\n",
    "        country = 'united states'\n",
    "\n",
    "    if country in country_list:\n",
    "        country_dist1 = dist_ling[dist_ling['country_i'] == country]\n",
    "        country_dist2 = distances[country]\n",
    "\n",
    "        train_indexes = list(recipe_dict1['Train_Variations'].keys())\n",
    "        valid_indexes = list(recipe_dict1['Valid_Variations'].keys())\n",
    "        test_indexes = list(recipe_dict1['Test_Variations'].keys())\n",
    "\n",
    "        if len(train_indexes) > 1:\n",
    "            newness_div, newness_prob, new_extremes, new_rank, uniq_dist, uniq_proto, diff_global, diff_local, new_surprise, dist_surprise = get_novel_scores(recipe_dict2['Train_Variations'], train_indexes)\n",
    "            dist_ingle = get_distance(recipe_dict2['Train_Variations'], train_indexes, country_dist2, type='ingle')\n",
    "            dist_geo = get_distance(recipe_dict2['Train_Variations'], train_indexes, country_dist2, type='geo')\n",
    "            dist_ling = get_distance(recipe_dict2['Train_Variations'], train_indexes, country_dist1, type='ling')\n",
    "            dist_reli = get_distance(recipe_dict2['Train_Variations'], train_indexes, country_dist1, type='reli')\n",
    "            nb_ingr, nb_new_ingr, ratio_new_ingr, clean_text_length, clean_nb_uniq_tokens, lexical_div, ratio_length = get_metadata(recipe_dict1['Train_Variations'], KB_avg_length, train_indexes)\n",
    "            \n",
    "            all_new_div.append(newness_div)\n",
    "            all_new_prob.append(newness_prob)\n",
    "            all_new_extr.append(new_extremes)\n",
    "            all_new_rank.append(new_rank)\n",
    "            all_uniq_dist.append(uniq_dist)\n",
    "            all_uniq_proto.append(uniq_proto)\n",
    "            all_diff_glob.append(diff_global)\n",
    "            all_diff_loc.append(diff_local)\n",
    "            all_nsurp.append(new_surprise)\n",
    "            all_dsurp.append(dist_surprise)\n",
    "            all_dist_ingle.append(dist_ingle)\n",
    "            all_dist_ling.append(dist_ling)\n",
    "            all_dist_geo.append(dist_geo)\n",
    "            all_dist_rel.append(dist_reli)\n",
    "            nb_ingr.append(nb_ingr)\n",
    "            nb_new_ingr.append(nb_new_ingr)\n",
    "            ratio_new_ingr.append(ratio_new_ingr)\n",
    "            clean_text_length.append(clean_text_length)\n",
    "            clean_nb_uniq_tokens.append(clean_nb_uniq_tokens)\n",
    "            lexical_div.append(lexical_div)\n",
    "            ratio_length.append(ratio_length)\n",
    "\n",
    "        if len(valid_indexes) > 1:\n",
    "            newness_div, newness_prob, new_extremes, new_rank, uniq_dist, uniq_proto, diff_global, diff_local, new_surprise, dist_surprise = get_novel_scores(recipe_dict2['Valid_Variations'], valid_indexes)\n",
    "            dist_ingle = get_distance(recipe_dict2['Valid_Variations'], valid_indexes, country_dist2, type='ingle')\n",
    "            dist_geo = get_distance(recipe_dict2['Valid_Variations'], valid_indexes, country_dist2, type='geo')\n",
    "            dist_ling = get_distance(recipe_dict2['Valid_Variations'], valid_indexes, country_dist1, type='ling')\n",
    "            dist_reli = get_distance(recipe_dict2['Valid_Variations'], valid_indexes, country_dist1, type='reli')\n",
    "            nb_ingr, nb_new_ingr, ratio_new_ingr, clean_text_length, clean_nb_uniq_tokens, lexical_div, ratio_length = get_metadata(recipe_dict1['Valid_Variations'], KB_avg_length, valid_indexes)\n",
    "            \n",
    "            all_new_div.append(newness_div)\n",
    "            all_new_prob.append(newness_prob)\n",
    "            all_new_extr.append(new_extremes)\n",
    "            all_new_rank.append(new_rank)\n",
    "            all_uniq_dist.append(uniq_dist)\n",
    "            all_uniq_proto.append(uniq_proto)\n",
    "            all_diff_glob.append(diff_global)\n",
    "            all_diff_loc.append(diff_local)\n",
    "            all_nsurp.append(new_surprise)\n",
    "            all_dsurp.append(dist_surprise)\n",
    "            all_dist_ingle.append(dist_ingle)\n",
    "            all_dist_ling.append(dist_ling)\n",
    "            all_dist_geo.append(dist_geo)\n",
    "            all_dist_rel.append(dist_reli)\n",
    "            nb_ingr.append(nb_ingr)\n",
    "            nb_new_ingr.append(nb_new_ingr)\n",
    "            ratio_new_ingr.append(ratio_new_ingr)\n",
    "            clean_text_length.append(clean_text_length)\n",
    "            clean_nb_uniq_tokens.append(clean_nb_uniq_tokens)\n",
    "            lexical_div.append(lexical_div)\n",
    "            ratio_length.append(ratio_length)\n",
    "\n",
    "        if len(test_indexes) > 1:\n",
    "            newness_div, newness_prob, new_extremes, new_rank, uniq_dist, uniq_proto, diff_global, diff_local, new_surprise, dist_surprise = get_novel_scores(recipe_dict2['Test_Variations'], test_indexes)\n",
    "            dist_ingle = get_distance(recipe_dict2['Test_Variations'], test_indexes, country_dist2, type='ingle')\n",
    "            dist_geo = get_distance(recipe_dict2['Test_Variations'], test_indexes, country_dist2, type='geo')\n",
    "            dist_ling = get_distance(recipe_dict2['Test_Variations'], test_indexes, country_dist1, type='ling')\n",
    "            dist_reli = get_distance(recipe_dict2['Test_Variations'], test_indexes, country_dist1, type='reli')\n",
    "            nb_ingr, nb_new_ingr, ratio_new_ingr, clean_text_length, clean_nb_uniq_tokens, lexical_div, ratio_length = get_metadata(recipe_dict1['Test_Variations'], KB_avg_length, test_indexes)\n",
    "            \n",
    "            all_new_div.append(newness_div)\n",
    "            all_new_prob.append(newness_prob)\n",
    "            all_new_extr.append(new_extremes)\n",
    "            all_new_rank.append(new_rank)\n",
    "            all_uniq_dist.append(uniq_dist)\n",
    "            all_uniq_proto.append(uniq_proto)\n",
    "            all_diff_glob.append(diff_global)\n",
    "            all_diff_loc.append(diff_local)\n",
    "            all_nsurp.append(new_surprise)\n",
    "            all_dsurp.append(dist_surprise)\n",
    "            all_dist_ingle.append(dist_ingle)\n",
    "            all_dist_ling.append(dist_ling)\n",
    "            all_dist_geo.append(dist_geo)\n",
    "            all_dist_rel.append(dist_reli)\n",
    "            nb_ingr.append(nb_ingr)\n",
    "            nb_new_ingr.append(nb_new_ingr)\n",
    "            ratio_new_ingr.append(ratio_new_ingr)\n",
    "            clean_text_length.append(clean_text_length)\n",
    "            clean_nb_uniq_tokens.append(clean_nb_uniq_tokens)\n",
    "            lexical_div.append(lexical_div)\n",
    "            ratio_length.append(ratio_length)\n",
    "\n",
    "\n",
    "data = {\n",
    "    'newness_div': all_new_div,\n",
    "    'newness_prob': all_new_prob,\n",
    "    'new_extremes': all_new_extr,\n",
    "    'new_rank': all_new_rank,\n",
    "    'uniq_dist': all_uniq_dist,\n",
    "    'uniq_proto': all_uniq_proto,\n",
    "    'diff_global': all_diff_glob,\n",
    "    'diff_local': all_diff_loc,\n",
    "    'new_surprise': all_nsurp,\n",
    "    'dist_surprise': all_dsurp,\n",
    "    'dist_ingle': all_dist_ingle,\n",
    "    'dist_ling': all_dist_ling,\n",
    "    'dist_geo': all_dist_geo,\n",
    "    'dist_reli': all_dist_rel,\n",
    "    'nb_ingr': nb_ingr,\n",
    "    'nb_new_ingr': nb_new_ingr,\n",
    "    'ratio_new_ingr': ratio_new_ingr,\n",
    "    'clean_text_length': clean_text_length,\n",
    "    'clean_nb_uniq_tokens': clean_nb_uniq_tokens,\n",
    "    'lexical_div': lexical_div,\n",
    "    'ratio_length': ratio_length\n",
    "}\n",
    "\n",
    "# Filter out keys where the list is empty\n",
    "filtered_data = {k: v for k, v in data.items() if len(v) > 0}\n",
    "df = pd.DataFrame(filtered_data)\n",
    "# Save to CSV\n",
    "df.to_csv(f\"{ROOT_DIR}/datasets/GlobalFusion/correlations_regressions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672f557b-c69c-4ffa-8161-1ef01de4aa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### YOU JUST NOW NEED TO APPLY CLASSIC RBO, KENDALL TAU, REGRESSION, OR PEARSON ANALYSES FROM THAT FILE\n",
    "#### YOU CAN DO it ;)\n",
    "### HERE THE LIBRARIES USED FOR ANALYSIS\n",
    "from correlations_utils import *    ## For rbo, kendall_tau correaltions\n",
    "\n",
    "## For pearson correaltions \n",
    "from scipy import stats\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "#For logistic regression analyses\n",
    "from scipy import stats\n",
    "import math\n",
    "import statistics\n",
    "from scipy.spatial import distance\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#For mediation analyses\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.mediation import Mediation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978b836-b2b3-47e4-9a6d-72c6aee21233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
